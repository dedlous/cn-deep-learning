{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 图像分类\n",
    "\n",
    "在此项目中，你将对 [CIFAR-10 数据集](https://www.cs.toronto.edu/~kriz/cifar.html) 中的图片进行分类。该数据集包含飞机、猫狗和其他物体。你需要预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。你需要应用所学的知识构建卷积的、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后，你需要在样本图片上看到神经网络的预测结果。\n",
    "\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "请运行以下单元，以下载 [CIFAR-10 数据集（Python版）](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "\n",
    "该数据集分成了几部分／批次（batches），以免你的机器在计算时内存不足。CIFAR-10 数据集包含 5 个部分，名称分别为 `data_batch_1`、`data_batch_2`，以此类推。每个部分都包含以下某个类别的标签和图片：\n",
    "\n",
    "* 飞机\n",
    "* 汽车\n",
    "* 鸟类\n",
    "* 猫\n",
    "* 鹿\n",
    "* 狗\n",
    "* 青蛙\n",
    "* 马\n",
    "* 船只\n",
    "* 卡车\n",
    "\n",
    "了解数据集也是对数据进行预测的必经步骤。你可以通过更改 `batch_id` 和 `sample_id` 探索下面的代码单元。`batch_id` 是数据集一个部分的 ID（1 到 5）。`sample_id` 是该部分中图片和标签对（label pair）的 ID。\n",
    "\n",
    "问问你自己：“可能的标签有哪些？”、“图片数据的值范围是多少？”、“标签是按顺序排列，还是随机排列的？”。思考类似的问题，有助于你预处理数据，并使预测结果更准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 600:\n",
      "Image - Min Value: 24 Max Value: 242\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 0 Name: airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4IeABoT9ADQmKAHgMb+NzQcVS6Cr4XvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x49ed5c0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 600\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "\n",
    "### 标准化\n",
    "\n",
    "在下面的单元中，实现 `normalize` 函数，传入图片数据 `x`，并返回标准化 Numpy 数组。值应该在 0 到 1 的范围内（含 0 和 1）。返回对象应该和 `x` 的形状一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498, 32, 32, 3)\n",
      "[[ 37 233  56]\n",
      " [209  78  86]\n",
      " [ 96   5 214]\n",
      " [ 94 188 215]\n",
      " [236 198  83]\n",
      " [ 74 137  10]\n",
      " [116 175  72]\n",
      " [234 171 173]\n",
      " [ 59  35 108]\n",
      " [132  61 202]\n",
      " [213 130 221]\n",
      " [ 37 145 199]\n",
      " [114 142  50]\n",
      " [ 31 208  32]\n",
      " [251  92  90]\n",
      " [ 71  58 114]\n",
      " [ 62 216   0]\n",
      " [ 94  14 177]\n",
      " [ 64 105 163]\n",
      " [ 23 120  45]\n",
      " [189  45 238]\n",
      " [ 36 106  71]\n",
      " [ 39  45 239]\n",
      " [  9   8  80]\n",
      " [ 39 225 148]\n",
      " [244  33 169]\n",
      " [250 243 253]\n",
      " [182 202  29]\n",
      " [188   3 238]\n",
      " [ 89  35 162]\n",
      " [156 247  81]\n",
      " [118  56  18]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    print (x.shape)\n",
    "    print (x[0,10])\n",
    "    return x/256.0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot 编码\n",
    "\n",
    "和之前的代码单元一样，你将为预处理实现一个函数。这次，你将实现 `one_hot_encode` 函数。输入，也就是 `x`，是一个标签列表。实现该函数，以返回为 one_hot 编码的 Numpy 数组的标签列表。标签的可能值为 0 到 9。每次调用 `one_hot_encode` 时，对于每个值，one_hot 编码函数应该返回相同的编码。确保将编码映射保存到该函数外面。\n",
    "\n",
    "提示：不要重复发明轮子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing \n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     print (x.shape)\n",
    "#     enc = preprocessing.OneHotEncoder()\n",
    "#     enc.fit(x)\n",
    "#     one_hot=transform(x)\n",
    "    labels=list(range(10))\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(labels)\n",
    "\n",
    "    return np.array(lb.transform(x))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机化数据\n",
    "\n",
    "之前探索数据时，你已经了解到，样本的顺序是随机的。再随机化一次也不会有什么关系，但是对于这个数据集没有必要。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理所有数据并保存\n",
    "\n",
    "运行下方的代码单元，将预处理所有 CIFAR-10 数据，并保存到文件中。下面的代码还使用了 10% 的训练数据，用来验证。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 32, 32, 3)\n",
      "[[154 122  94]\n",
      " [155 117  82]\n",
      " [156 117  82]\n",
      " [147 108  70]\n",
      " [133 100  64]\n",
      " [137 100  66]\n",
      " [139 102  68]\n",
      " [134 102  66]\n",
      " [141 111  81]\n",
      " [121  87  68]\n",
      " [ 80  40  13]\n",
      " [ 97  53  17]\n",
      " [ 90  45  17]\n",
      " [ 98  56  30]\n",
      " [137  91  57]\n",
      " [139  84  49]\n",
      " [148  87  54]\n",
      " [134  73  37]\n",
      " [138  82  46]\n",
      " [134  85  57]\n",
      " [140  92  76]\n",
      " [175 129 106]\n",
      " [142  99  53]\n",
      " [102  61  26]\n",
      " [108  67  25]\n",
      " [135  90  41]\n",
      " [131  87  45]\n",
      " [133  91  51]\n",
      " [138  97  57]\n",
      " [136  95  55]\n",
      " [130  86  46]\n",
      " [134  93  57]]\n",
      "(9000, 32, 32, 3)\n",
      "[[128 125 119]\n",
      " [107 109 105]\n",
      " [110 114 119]\n",
      " [ 84  85  97]\n",
      " [ 76  76  93]\n",
      " [ 82  84 107]\n",
      " [ 99 103 129]\n",
      " [107 115 141]\n",
      " [102 115 146]\n",
      " [102 118 151]\n",
      " [104 117 149]\n",
      " [103 113 152]\n",
      " [101 113 151]\n",
      " [ 97 113 149]\n",
      " [100 117 151]\n",
      " [103 119 151]\n",
      " [ 89 105 137]\n",
      " [ 57  73 105]\n",
      " [ 77  95 126]\n",
      " [ 85 104 136]\n",
      " [ 84 103 132]\n",
      " [ 82 102 128]\n",
      " [ 78  97 123]\n",
      " [100 115 141]\n",
      " [ 97 113 136]\n",
      " [ 86 103 123]\n",
      " [ 85  99 118]\n",
      " [ 94 102 122]\n",
      " [ 92 100 119]\n",
      " [ 84  94 103]\n",
      " [ 62  72  71]\n",
      " [ 64  71  67]]\n",
      "(9000, 32, 32, 3)\n",
      "[[ 53  59  72]\n",
      " [ 20  23  37]\n",
      " [ 14  13  28]\n",
      " [ 19  21  40]\n",
      " [ 73  77  97]\n",
      " [181 187 202]\n",
      " [184 190 203]\n",
      " [ 91  95 106]\n",
      " [ 18  19  30]\n",
      " [ 11   9  25]\n",
      " [  9   5  17]\n",
      " [ 22  18  21]\n",
      " [ 87  82  76]\n",
      " [122 115 105]\n",
      " [165 157 141]\n",
      " [227 218 200]\n",
      " [253 250 242]\n",
      " [247 246 245]\n",
      " [232 232 234]\n",
      " [142 144 152]\n",
      " [145 150 154]\n",
      " [184 190 188]\n",
      " [222 225 224]\n",
      " [248 248 247]\n",
      " [237 236 238]\n",
      " [229 228 231]\n",
      " [220 220 221]\n",
      " [102 101 107]\n",
      " [ 10   8  21]\n",
      " [ 19  19  31]\n",
      " [ 16  16  29]\n",
      " [ 15  14  30]]\n",
      "(9000, 32, 32, 3)\n",
      "[[115 107 116]\n",
      " [126 118 125]\n",
      " [114 110 117]\n",
      " [107 108 113]\n",
      " [131 129 133]\n",
      " [137 133 137]\n",
      " [120 117 120]\n",
      " [123 120 120]\n",
      " [118 116 116]\n",
      " [105 101 105]\n",
      " [119 109 113]\n",
      " [121 115 117]\n",
      " [112 108 112]\n",
      " [114 112 115]\n",
      " [123 118 122]\n",
      " [126 116 121]\n",
      " [110 107 111]\n",
      " [105 104 108]\n",
      " [113 110 113]\n",
      " [135 120 125]\n",
      " [140 122 128]\n",
      " [131 117 123]\n",
      " [118 116 120]\n",
      " [114 114 117]\n",
      " [ 85  90  94]\n",
      " [ 89  93  97]\n",
      " [ 93  97 100]\n",
      " [ 90  95  99]\n",
      " [ 81  88  93]\n",
      " [ 97 100 106]\n",
      " [107 107 112]\n",
      " [102 103 107]]\n",
      "(9000, 32, 32, 3)\n",
      "[[255 255 255]\n",
      " [253 253 252]\n",
      " [255 255 255]\n",
      " [253 252 244]\n",
      " [207 205 189]\n",
      " [211 209 193]\n",
      " [242 242 227]\n",
      " [216 218 202]\n",
      " [177 195 183]\n",
      " [110 158 157]\n",
      " [106 163 163]\n",
      " [ 76 126 124]\n",
      " [ 61 113 121]\n",
      " [ 27  77  87]\n",
      " [ 24  69  79]\n",
      " [ 35  79  88]\n",
      " [ 39  91  94]\n",
      " [ 28  87  84]\n",
      " [ 70 134 127]\n",
      " [103 172 160]\n",
      " [128 196 180]\n",
      " [169 221 208]\n",
      " [195 220 214]\n",
      " [227 230 228]\n",
      " [255 253 254]\n",
      " [255 254 255]\n",
      " [255 254 255]\n",
      " [255 254 255]\n",
      " [255 254 255]\n",
      " [255 253 254]\n",
      " [255 254 251]\n",
      " [254 254 252]]\n",
      "(5000, 32, 32, 3)\n",
      "[[177 153 176]\n",
      " [180 156 185]\n",
      " [182 158 190]\n",
      " [177 155 184]\n",
      " [177 157 181]\n",
      " [188 166 194]\n",
      " [186 161 198]\n",
      " [193 168 206]\n",
      " [199 173 207]\n",
      " [188 162 191]\n",
      " [198 174 195]\n",
      " [197 171 190]\n",
      " [197 164 192]\n",
      " [193 158 185]\n",
      " [188 157 174]\n",
      " [190 160 171]\n",
      " [191 163 174]\n",
      " [204 177 192]\n",
      " [207 184 209]\n",
      " [216 193 227]\n",
      " [219 188 216]\n",
      " [166 140 158]\n",
      " [164 147 155]\n",
      " [180 162 163]\n",
      " [169 141 140]\n",
      " [181 155 157]\n",
      " [191 168 171]\n",
      " [165 141 142]\n",
      " [162 138 136]\n",
      " [191 168 164]\n",
      " [187 161 162]\n",
      " [185 158 160]]\n",
      "(10000, 32, 32, 3)\n",
      "[[183 183 175]\n",
      " [108 116 122]\n",
      " [142 151 158]\n",
      " [165 169 168]\n",
      " [177 156 122]\n",
      " [155 112  50]\n",
      " [159 118  51]\n",
      " [122  89  47]\n",
      " [213 197 179]\n",
      " [237 224 226]\n",
      " [220 191 188]\n",
      " [164 135 131]\n",
      " [183 159 155]\n",
      " [156 137 132]\n",
      " [125 108 104]\n",
      " [120 111 104]\n",
      " [ 78  76  69]\n",
      " [ 80  80  77]\n",
      " [ 45  44  40]\n",
      " [ 91  85  77]\n",
      " [175 165 154]\n",
      " [157 147 137]\n",
      " [155 147 138]\n",
      " [107 100  92]\n",
      " [ 87  83  77]\n",
      " [103 102  96]\n",
      " [ 88  88  79]\n",
      " [ 78  79  73]\n",
      " [ 59  59  59]\n",
      " [ 41  36  33]\n",
      " [ 59  46  31]\n",
      " [104  81  46]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "这是你的第一个检查点。如果你什么时候决定再回到该记事本，或需要重新启动该记事本，你可以从这里开始。预处理的数据已保存到本地。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "对于该神经网络，你需要将每层都构建为一个函数。你看到的大部分代码都位于函数外面。要更全面地测试你的代码，我们需要你将每层放入一个函数中。这样使我们能够提供更好的反馈，并使用我们的统一测试检测简单的错误，然后再提交项目。\n",
    "\n",
    ">**注意**：如果你觉得每周很难抽出足够的时间学习这门课程，我们为此项目提供了一个小捷径。对于接下来的几个问题，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 程序包中的类来构建每个层级，但是“卷积和最大池化层级”部分的层级除外。TF Layers 和 Keras 及 TFLearn 层级类似，因此很容易学会。\n",
    "\n",
    ">但是，如果你想充分利用这门课程，请尝试自己解决所有问题，不使用 TF Layers 程序包中的任何类。你依然可以使用其他程序包中的类，这些类和你在 TF Layers 中的类名称是一样的！例如，你可以使用 TF Neural Network 版本的 `conv2d` 类 [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)，而不是 TF Layers 版本的 `conv2d` 类 [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d)。\n",
    "\n",
    "我们开始吧！\n",
    "\n",
    "\n",
    "### 输入\n",
    "\n",
    "神经网络需要读取图片数据、one-hot 编码标签和丢弃保留概率（dropout keep probability）。请实现以下函数：\n",
    "\n",
    "* 实现 `neural_net_image_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `image_shape` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"x\" 命名\n",
    "* 实现 `neural_net_label_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `n_classes` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"y\" 命名\n",
    "* 实现 `neural_net_keep_prob_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)，用于丢弃保留概率\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"keep_prob\" 命名\n",
    "\n",
    "这些名称将在项目结束时，用于加载保存的模型。\n",
    "\n",
    "注意：TensorFlow 中的 `None` 表示形状可以是动态大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "32 32 3\n",
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    print(image_shape)\n",
    "    image_height, image_width, color_channels=image_shape\n",
    "    print (image_height, image_width, color_channels)\n",
    "#     input = tf.placeholder(tf.float32,shape=[None, *image_shape],name='x')\n",
    "    input=tf.placeholder(tf.float32, shape=[None, image_height, image_width, color_channels], name=\"x\")\n",
    "    \n",
    "    return input\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return  tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和最大池化层\n",
    "\n",
    "卷积层级适合处理图片。对于此代码单元，你应该实现函数 `conv2d_maxpool` 以便应用卷积然后进行最大池化：\n",
    "\n",
    "* 使用 `conv_ksize`、`conv_num_outputs` 和 `x_tensor` 的形状创建权重（weight）和偏置（bias）。\n",
    "* 使用权重和 `conv_strides` 对 `x_tensor` 应用卷积。\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "* 添加偏置\n",
    "* 向卷积中添加非线性激活（nonlinear activation）\n",
    "* 使用 `pool_ksize` 和 `pool_strides` 应用最大池化\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "\n",
    "**注意**：对于**此层**，**请勿使用** [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers)，但是仍然可以使用 TensorFlow 的 [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) 包。对于所有**其他层**，你依然可以使用快捷方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 5)\n",
      "10\n",
      "5\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    print (x_tensor.shape)\n",
    "    print (conv_num_outputs)\n",
    "    color_channels=x_tensor.shape[3].value\n",
    "    print (color_channels)\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal(shape=\n",
    "    [conv_ksize[0], conv_ksize[1], color_channels, conv_num_outputs],mean=0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    conv_layer = tf.nn.max_pool(conv_layer,ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1],padding='SAME')\n",
    "    \n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扁平化层\n",
    "\n",
    "实现 `flatten` 函数，将 `x_tensor` 的维度从四维张量（4-D tensor）变成二维张量。输出应该是形状（*部分大小（Batch Size）*，*扁平化图片大小（Flattened Image Size）*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "\n",
    "实现 `fully_conn` 函数，以向 `x_tensor` 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    print (x_tensor.shape)\n",
    "    shape = x_tensor.shape[1].value\n",
    "    weights = tf.Variable(tf.truncated_normal([shape, num_outputs], mean=0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "\n",
    "    layer = tf.matmul(x_tensor, weights)\n",
    "\n",
    "    layer = tf.nn.bias_add(layer, bias)\n",
    "    \n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层\n",
    "\n",
    "实现 `output` 函数，向 x_tensor 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n",
    "\n",
    "**注意**：该层级不应应用 Activation、softmax 或交叉熵（cross entropy）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = x_tensor.shape[1].value\n",
    "    weights = tf.Variable(tf.truncated_normal([shape, num_outputs], mean=0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "\n",
    "    layer = tf.matmul(x_tensor, weights)\n",
    "\n",
    "    layer = tf.nn.bias_add(layer, bias)\n",
    "    \n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建卷积模型\n",
    "\n",
    "实现函数 `conv_net`， 创建卷积神经网络模型。该函数传入一批图片 `x`，并输出对数（logits）。使用你在上方创建的层创建此模型：\n",
    "\n",
    "* 应用 1、2 或 3 个卷积和最大池化层（Convolution and Max Pool layers）\n",
    "* 应用一个扁平层（Flatten Layer）\n",
    "* 应用 1、2 或 3 个完全连接层（Fully Connected Layers）\n",
    "* 应用一个输出层（Output Layer）\n",
    "* 返回输出\n",
    "* 使用 `keep_prob` 向模型中的一个或多个层应用 [TensorFlow 的 Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "32 32 3\n",
      "(?, 32, 32, 3)\n",
      "16\n",
      "3\n",
      "(?, 16, 16, 16)\n",
      "32\n",
      "16\n",
      "(?, 8, 8, 32)\n",
      "64\n",
      "32\n",
      "debug_mark11\n",
      "debug_mark2\n",
      "(?, 1024)\n",
      "debug_mark3\n",
      "(?, 32, 32, 3)\n",
      "16\n",
      "3\n",
      "(?, 16, 16, 16)\n",
      "32\n",
      "16\n",
      "(?, 8, 8, 32)\n",
      "64\n",
      "32\n",
      "debug_mark11\n",
      "debug_mark2\n",
      "(?, 1024)\n",
      "debug_mark3\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_ksize = (3,3)\n",
    "    conv_strides = (2,2)\n",
    "    pool_ksize = (3,3)\n",
    "    pool_strides = (1,1)\n",
    "    \n",
    "    conv2d_maxpool_layer_1 = conv2d_maxpool(x, 16, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv2d_maxpool_layer_2 = conv2d_maxpool(conv2d_maxpool_layer_1, 32, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv2d_maxpool_layer_3 = conv2d_maxpool(conv2d_maxpool_layer_2, 64, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    print (\"debug_mark11\")\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    flatten_layer = flatten(conv2d_maxpool_layer_3)\n",
    "    print (\"debug_mark2\")\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    fully_conn_layer = fully_conn(flatten_layer, 32)\n",
    "    fully_conn_layer = tf.nn.dropout(fully_conn_layer, keep_prob)\n",
    "    print (\"debug_mark3\")\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    output_layer = output(fully_conn_layer, 10)\n",
    "    # TODO: return output\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "### 单次优化\n",
    "\n",
    "实现函数 `train_neural_network` 以进行单次优化（single optimization）。该优化应该使用 `optimizer` 优化 `session`，其中 `feed_dict` 具有以下参数：\n",
    "\n",
    "* `x` 表示图片输入\n",
    "* `y` 表示标签\n",
    "* `keep_prob` 表示丢弃的保留率\n",
    "\n",
    "每个部分都会调用该函数，所以 `tf.global_variables_initializer()` 已经被调用。\n",
    "\n",
    "注意：不需要返回任何内容。该函数只是用来优化神经网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x:feature_batch,y:label_batch,keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据\n",
    "\n",
    "实现函数 `print_stats` 以输出损失和验证准确率。使用全局变量 `valid_features` 和 `valid_labels` 计算验证准确率。使用保留率 `1.0` 计算损失和验证准确率（loss and validation accuracy）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    global valid_features, valid_labels\n",
    "    loss = session.run(cost,feed_dict={x:feature_batch,y:label_batch,keep_prob:1.0})\n",
    "    acc = session.run(accuracy, feed_dict={x:valid_features,y:valid_labels,keep_prob:1.0})\n",
    "    \n",
    "    print('loss = {0} -  accuracy = {1}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数\n",
    "\n",
    "调试以下超参数：\n",
    "* 设置 `epochs` 表示神经网络停止学习或开始过拟合的迭代次数\n",
    "* 设置 `batch_size`，表示机器内存允许的部分最大体积。大部分人设为以下常见内存大小：\n",
    "\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* 设置 `keep_probability` 表示使用丢弃时保留节点的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在单个 CIFAR-10 部分上训练\n",
    "\n",
    "我们先用单个部分，而不是用所有的 CIFAR-10 批次训练神经网络。这样可以节省时间，并对模型进行迭代，以提高准确率。最终验证准确率达到 50% 或以上之后，在下一部分对所有数据运行模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss = 2.291900873184204 -  accuracy = 0.10980000346899033\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss = 2.2711267471313477 -  accuracy = 0.15620000660419464\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss = 2.2275547981262207 -  accuracy = 0.2062000036239624\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss = 2.144949197769165 -  accuracy = 0.21279999613761902\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss = 2.0993213653564453 -  accuracy = 0.25040000677108765\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss = 2.0069310665130615 -  accuracy = 0.31040000915527344\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss = 1.9603956937789917 -  accuracy = 0.3368000090122223\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss = 1.9047552347183228 -  accuracy = 0.34119999408721924\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss = 1.8723331689834595 -  accuracy = 0.3440000116825104\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss = 1.8558096885681152 -  accuracy = 0.35100001096725464\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss = 1.8097708225250244 -  accuracy = 0.35899999737739563\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss = 1.7723567485809326 -  accuracy = 0.37779998779296875\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss = 1.7478492259979248 -  accuracy = 0.3776000142097473\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss = 1.7382556200027466 -  accuracy = 0.382999986410141\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss = 1.6897180080413818 -  accuracy = 0.39480000734329224\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss = 1.6645156145095825 -  accuracy = 0.4074000120162964\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss = 1.6590204238891602 -  accuracy = 0.39959999918937683\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss = 1.6173430681228638 -  accuracy = 0.4083999991416931\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss = 1.5967271327972412 -  accuracy = 0.415800005197525\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss = 1.5763752460479736 -  accuracy = 0.41819998621940613\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss = 1.5598313808441162 -  accuracy = 0.4230000078678131\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss = 1.5522862672805786 -  accuracy = 0.4269999861717224\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss = 1.5238748788833618 -  accuracy = 0.4323999881744385\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss = 1.5341657400131226 -  accuracy = 0.430400013923645\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss = 1.5062335729599 -  accuracy = 0.43059998750686646\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss = 1.4924449920654297 -  accuracy = 0.4399999976158142\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss = 1.4798986911773682 -  accuracy = 0.4390000104904175\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss = 1.4921995401382446 -  accuracy = 0.43540000915527344\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss = 1.4796878099441528 -  accuracy = 0.44339999556541443\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss = 1.4744070768356323 -  accuracy = 0.4408000111579895\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss = 1.4473787546157837 -  accuracy = 0.43959999084472656\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss = 1.4838504791259766 -  accuracy = 0.43799999356269836\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss = 1.4204853773117065 -  accuracy = 0.4519999921321869\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss = 1.4396677017211914 -  accuracy = 0.45320001244544983\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss = 1.4142475128173828 -  accuracy = 0.4546000063419342\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss = 1.4039641618728638 -  accuracy = 0.45840001106262207\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss = 1.405599594116211 -  accuracy = 0.46540001034736633\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss = 1.396368384361267 -  accuracy = 0.4620000123977661\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss = 1.3925458192825317 -  accuracy = 0.4699999988079071\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss = 1.404934287071228 -  accuracy = 0.4593999981880188\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss = 1.409462809562683 -  accuracy = 0.4564000070095062\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss = 1.369834065437317 -  accuracy = 0.4690000116825104\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss = 1.3757100105285645 -  accuracy = 0.47279998660087585\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss = 1.3799282312393188 -  accuracy = 0.4659999907016754\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss = 1.364679217338562 -  accuracy = 0.4731999933719635\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss = 1.352419137954712 -  accuracy = 0.4713999927043915\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss = 1.346371054649353 -  accuracy = 0.48399999737739563\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss = 1.3591994047164917 -  accuracy = 0.47040000557899475\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss = 1.3406469821929932 -  accuracy = 0.4781999886035919\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss = 1.3294297456741333 -  accuracy = 0.49160000681877136\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss = 1.3404525518417358 -  accuracy = 0.48019999265670776\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss = 1.3147081136703491 -  accuracy = 0.4912000000476837\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss = 1.3254995346069336 -  accuracy = 0.48739999532699585\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss = 1.3135244846343994 -  accuracy = 0.49140000343322754\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss = 1.316718339920044 -  accuracy = 0.49140000343322754\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss = 1.2977277040481567 -  accuracy = 0.4968000054359436\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss = 1.287724256515503 -  accuracy = 0.49480000138282776\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss = 1.3074700832366943 -  accuracy = 0.49059998989105225\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss = 1.286934733390808 -  accuracy = 0.49619999527931213\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss = 1.2849177122116089 -  accuracy = 0.4925999939441681\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss = 1.2783254384994507 -  accuracy = 0.5016000270843506\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss = 1.292033076286316 -  accuracy = 0.4973999857902527\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss = 1.297701120376587 -  accuracy = 0.4952000081539154\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss = 1.2782588005065918 -  accuracy = 0.5072000026702881\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss = 1.2767536640167236 -  accuracy = 0.4986000061035156\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss = 1.2660397291183472 -  accuracy = 0.5063999891281128\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss = 1.2709184885025024 -  accuracy = 0.5073999762535095\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss = 1.2527676820755005 -  accuracy = 0.503600001335144\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss = 1.2591217756271362 -  accuracy = 0.5062000155448914\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss = 1.2545162439346313 -  accuracy = 0.5052000284194946\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss = 1.232271432876587 -  accuracy = 0.5144000053405762\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss = 1.2419259548187256 -  accuracy = 0.5135999917984009\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss = 1.2265063524246216 -  accuracy = 0.5157999992370605\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss = 1.2435611486434937 -  accuracy = 0.5072000026702881\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss = 1.229215145111084 -  accuracy = 0.5073999762535095\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss = 1.2182270288467407 -  accuracy = 0.5167999863624573\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss = 1.235514521598816 -  accuracy = 0.5049999952316284\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss = 1.2200173139572144 -  accuracy = 0.515999972820282\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss = 1.1931480169296265 -  accuracy = 0.527999997138977\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss = 1.206695795059204 -  accuracy = 0.5157999992370605\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss = 1.2112292051315308 -  accuracy = 0.515999972820282\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss = 1.2004978656768799 -  accuracy = 0.5231999754905701\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss = 1.1694085597991943 -  accuracy = 0.5242000222206116\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss = 1.1788111925125122 -  accuracy = 0.5252000093460083\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss = 1.1794377565383911 -  accuracy = 0.5231999754905701\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss = 1.183962345123291 -  accuracy = 0.5153999924659729\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss = 1.1559842824935913 -  accuracy = 0.5320000052452087\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss = 1.1505800485610962 -  accuracy = 0.5285999774932861\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss = 1.184374213218689 -  accuracy = 0.5210000276565552\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss = 1.1798503398895264 -  accuracy = 0.5184000134468079\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss = 1.136198878288269 -  accuracy = 0.527400016784668\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss = 1.1555160284042358 -  accuracy = 0.5314000248908997\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss = 1.1535154581069946 -  accuracy = 0.524399995803833\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss = 1.1622024774551392 -  accuracy = 0.5264000296592712\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss = 1.1440788507461548 -  accuracy = 0.5342000126838684\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss = 1.119834303855896 -  accuracy = 0.5357999801635742\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss = 1.159683108329773 -  accuracy = 0.5216000080108643\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss = 1.1407150030136108 -  accuracy = 0.5320000052452087\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss = 1.1359658241271973 -  accuracy = 0.5333999991416931\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss = 1.1245089769363403 -  accuracy = 0.5317999720573425\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全训练模型\n",
    "\n",
    "现在，单个 CIFAR-10 部分的准确率已经不错了，试试所有五个部分吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss = 2.2988195419311523 -  accuracy = 0.13120000064373016\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss = 2.2828001976013184 -  accuracy = 0.131400004029274\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss = 2.2540621757507324 -  accuracy = 0.22579999268054962\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss = 2.1959052085876465 -  accuracy = 0.2386000007390976\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss = 2.1235105991363525 -  accuracy = 0.24560000002384186\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss = 2.0919137001037598 -  accuracy = 0.24480000138282776\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss = 2.0172789096832275 -  accuracy = 0.29159998893737793\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss = 2.0084846019744873 -  accuracy = 0.3001999855041504\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss = 1.946593165397644 -  accuracy = 0.3102000057697296\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss = 1.9072625637054443 -  accuracy = 0.32019999623298645\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss = 1.9445544481277466 -  accuracy = 0.32580000162124634\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss = 1.86378812789917 -  accuracy = 0.3441999852657318\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss = 1.8836114406585693 -  accuracy = 0.35499998927116394\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss = 1.8247910737991333 -  accuracy = 0.36880001425743103\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss = 1.7893846035003662 -  accuracy = 0.36980000138282776\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss = 1.8451220989227295 -  accuracy = 0.3677999973297119\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss = 1.7729800939559937 -  accuracy = 0.3831999897956848\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss = 1.7976596355438232 -  accuracy = 0.384799987077713\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss = 1.7265071868896484 -  accuracy = 0.3901999890804291\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss = 1.6886868476867676 -  accuracy = 0.3970000147819519\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss = 1.734477162361145 -  accuracy = 0.4016000032424927\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss = 1.677304983139038 -  accuracy = 0.4041999876499176\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss = 1.6535063982009888 -  accuracy = 0.41920000314712524\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss = 1.6045746803283691 -  accuracy = 0.42419999837875366\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss = 1.576577067375183 -  accuracy = 0.42980000376701355\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss = 1.5707379579544067 -  accuracy = 0.4336000084877014\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss = 1.5644105672836304 -  accuracy = 0.4410000145435333\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss = 1.4939812421798706 -  accuracy = 0.4429999887943268\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss = 1.5060675144195557 -  accuracy = 0.4503999948501587\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss = 1.5079256296157837 -  accuracy = 0.446399986743927\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss = 1.5247036218643188 -  accuracy = 0.45179998874664307\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss = 1.5155361890792847 -  accuracy = 0.46399998664855957\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss = 1.4493138790130615 -  accuracy = 0.45879998803138733\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss = 1.4831809997558594 -  accuracy = 0.45179998874664307\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss = 1.471349835395813 -  accuracy = 0.45719999074935913\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss = 1.4701306819915771 -  accuracy = 0.4674000144004822\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss = 1.4762235879898071 -  accuracy = 0.47699999809265137\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss = 1.4299993515014648 -  accuracy = 0.46639999747276306\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss = 1.4245952367782593 -  accuracy = 0.4675999879837036\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss = 1.4521574974060059 -  accuracy = 0.4668000042438507\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss = 1.433595895767212 -  accuracy = 0.4821999967098236\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss = 1.4647327661514282 -  accuracy = 0.48420000076293945\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss = 1.4070327281951904 -  accuracy = 0.46880000829696655\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss = 1.4179319143295288 -  accuracy = 0.4659999907016754\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss = 1.4280768632888794 -  accuracy = 0.4668000042438507\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss = 1.4177119731903076 -  accuracy = 0.48660001158714294\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss = 1.4481415748596191 -  accuracy = 0.47920000553131104\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss = 1.3786660432815552 -  accuracy = 0.4864000082015991\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss = 1.386385440826416 -  accuracy = 0.47920000553131104\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss = 1.4017056226730347 -  accuracy = 0.4787999987602234\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss = 1.4169405698776245 -  accuracy = 0.48919999599456787\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss = 1.4212685823440552 -  accuracy = 0.4867999851703644\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss = 1.3649150133132935 -  accuracy = 0.49239999055862427\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss = 1.3710638284683228 -  accuracy = 0.4862000048160553\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss = 1.392233967781067 -  accuracy = 0.477400004863739\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss = 1.3998231887817383 -  accuracy = 0.483599990606308\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss = 1.3980764150619507 -  accuracy = 0.49900001287460327\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss = 1.350405216217041 -  accuracy = 0.49320000410079956\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss = 1.3452706336975098 -  accuracy = 0.504800021648407\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss = 1.3648823499679565 -  accuracy = 0.49219998717308044\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss = 1.390793800354004 -  accuracy = 0.5005999803543091\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss = 1.3936797380447388 -  accuracy = 0.5001999735832214\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss = 1.34122896194458 -  accuracy = 0.4997999966144562\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss = 1.3340768814086914 -  accuracy = 0.4966000020503998\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss = 1.3568394184112549 -  accuracy = 0.49239999055862427\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss = 1.3547194004058838 -  accuracy = 0.5027999877929688\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss = 1.3741809129714966 -  accuracy = 0.5123999714851379\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss = 1.3161953687667847 -  accuracy = 0.5052000284194946\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss = 1.3393518924713135 -  accuracy = 0.49880000948905945\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss = 1.3476166725158691 -  accuracy = 0.5034000277519226\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss = 1.3338967561721802 -  accuracy = 0.5121999979019165\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss = 1.3825794458389282 -  accuracy = 0.49959999322891235\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss = 1.2894498109817505 -  accuracy = 0.5067999958992004\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss = 1.3261616230010986 -  accuracy = 0.5095999836921692\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss = 1.3134218454360962 -  accuracy = 0.5156000256538391\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss = 1.3381264209747314 -  accuracy = 0.5157999992370605\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss = 1.3392447233200073 -  accuracy = 0.5216000080108643\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss = 1.2859783172607422 -  accuracy = 0.5077999830245972\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss = 1.297456979751587 -  accuracy = 0.5117999911308289\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss = 1.3157551288604736 -  accuracy = 0.5171999931335449\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss = 1.306164264678955 -  accuracy = 0.524399995803833\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss = 1.343727946281433 -  accuracy = 0.5228000283241272\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss = 1.267778754234314 -  accuracy = 0.51419997215271\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss = 1.2886799573898315 -  accuracy = 0.5138000249862671\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss = 1.288558840751648 -  accuracy = 0.5175999999046326\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss = 1.2969584465026855 -  accuracy = 0.5297999978065491\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss = 1.321668267250061 -  accuracy = 0.5248000025749207\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss = 1.264101266860962 -  accuracy = 0.5120000243186951\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss = 1.257131576538086 -  accuracy = 0.531000018119812\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss = 1.2755812406539917 -  accuracy = 0.5231999754905701\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss = 1.283724069595337 -  accuracy = 0.5333999991416931\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss = 1.299325942993164 -  accuracy = 0.5335999727249146\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss = 1.2336969375610352 -  accuracy = 0.5302000045776367\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss = 1.2712593078613281 -  accuracy = 0.527999997138977\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss = 1.2711970806121826 -  accuracy = 0.5206000208854675\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss = 1.2838655710220337 -  accuracy = 0.5389999747276306\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss = 1.301015019416809 -  accuracy = 0.5306000113487244\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss = 1.2333674430847168 -  accuracy = 0.5221999883651733\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss = 1.2584398984909058 -  accuracy = 0.5267999768257141\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss = 1.248434066772461 -  accuracy = 0.5306000113487244\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss = 1.2626279592514038 -  accuracy = 0.5342000126838684\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss = 1.2789865732192993 -  accuracy = 0.5414000153541565\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss = 1.2136328220367432 -  accuracy = 0.5356000065803528\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss = 1.2354023456573486 -  accuracy = 0.5411999821662903\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss = 1.239534854888916 -  accuracy = 0.5311999917030334\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss = 1.2664620876312256 -  accuracy = 0.5343999862670898\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss = 1.2766444683074951 -  accuracy = 0.5493999719619751\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss = 1.2246543169021606 -  accuracy = 0.5246000289916992\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss = 1.223480463027954 -  accuracy = 0.5364000201225281\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss = 1.2267796993255615 -  accuracy = 0.5450000166893005\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss = 1.2410553693771362 -  accuracy = 0.5526000261306763\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss = 1.251286268234253 -  accuracy = 0.5428000092506409\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss = 1.208673357963562 -  accuracy = 0.5306000113487244\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss = 1.234096646308899 -  accuracy = 0.532800018787384\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss = 1.2146146297454834 -  accuracy = 0.5446000099182129\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss = 1.2376911640167236 -  accuracy = 0.5515999794006348\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss = 1.2596927881240845 -  accuracy = 0.5436000227928162\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss = 1.1922004222869873 -  accuracy = 0.5360000133514404\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss = 1.1984107494354248 -  accuracy = 0.5478000044822693\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss = 1.206600308418274 -  accuracy = 0.5478000044822693\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss = 1.2492324113845825 -  accuracy = 0.5475999712944031\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss = 1.2492176294326782 -  accuracy = 0.551800012588501\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss = 1.165686845779419 -  accuracy = 0.5483999848365784\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss = 1.2124767303466797 -  accuracy = 0.5371999740600586\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss = 1.1841585636138916 -  accuracy = 0.5454000234603882\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss = 1.2196719646453857 -  accuracy = 0.555400013923645\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss = 1.2197182178497314 -  accuracy = 0.5558000206947327\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss = 1.176611304283142 -  accuracy = 0.5419999957084656\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss = 1.1796168088912964 -  accuracy = 0.5472000241279602\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss = 1.189181923866272 -  accuracy = 0.5440000295639038\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss = 1.2274951934814453 -  accuracy = 0.5523999929428101\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss = 1.2353227138519287 -  accuracy = 0.5479999780654907\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss = 1.167387843132019 -  accuracy = 0.545199990272522\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss = 1.1881170272827148 -  accuracy = 0.5564000010490417\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss = 1.1776740550994873 -  accuracy = 0.5486000180244446\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss = 1.2086318731307983 -  accuracy = 0.5576000213623047\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss = 1.218612790107727 -  accuracy = 0.5564000010490417\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss = 1.1631792783737183 -  accuracy = 0.5526000261306763\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss = 1.1689375638961792 -  accuracy = 0.5562000274658203\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss = 1.1683778762817383 -  accuracy = 0.5565999746322632\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss = 1.1916577816009521 -  accuracy = 0.5609999895095825\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss = 1.2197322845458984 -  accuracy = 0.5493999719619751\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss = 1.1540950536727905 -  accuracy = 0.5447999835014343\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss = 1.15257728099823 -  accuracy = 0.5590000152587891\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss = 1.151656150817871 -  accuracy = 0.5582000017166138\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss = 1.1786175966262817 -  accuracy = 0.5636000037193298\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss = 1.211869239807129 -  accuracy = 0.5504000186920166\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss = 1.1256972551345825 -  accuracy = 0.5595999956130981\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss = 1.1524159908294678 -  accuracy = 0.5623999834060669\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss = 1.1558171510696411 -  accuracy = 0.5461999773979187\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss = 1.1856404542922974 -  accuracy = 0.5550000071525574\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss = 1.2141326665878296 -  accuracy = 0.5583999752998352\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss = 1.1355623006820679 -  accuracy = 0.5509999990463257\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss = 1.1497840881347656 -  accuracy = 0.5609999895095825\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss = 1.1371866464614868 -  accuracy = 0.5616000294685364\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss = 1.1916382312774658 -  accuracy = 0.5605999827384949\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss = 1.1832250356674194 -  accuracy = 0.5669999718666077\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss = 1.1349304914474487 -  accuracy = 0.5562000274658203\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss = 1.1424143314361572 -  accuracy = 0.5666000247001648\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss = 1.1396658420562744 -  accuracy = 0.5595999956130981\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss = 1.1698960065841675 -  accuracy = 0.5667999982833862\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss = 1.1845271587371826 -  accuracy = 0.5663999915122986\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss = 1.1097017526626587 -  accuracy = 0.5654000043869019\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss = 1.117045521736145 -  accuracy = 0.5669999718666077\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss = 1.1298956871032715 -  accuracy = 0.5673999786376953\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss = 1.1649469137191772 -  accuracy = 0.5609999895095825\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss = 1.1776463985443115 -  accuracy = 0.5654000043869019\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss = 1.122843623161316 -  accuracy = 0.5587999820709229\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss = 1.1241551637649536 -  accuracy = 0.5637999773025513\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss = 1.1183210611343384 -  accuracy = 0.569599986076355\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss = 1.144964575767517 -  accuracy = 0.574400007724762\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss = 1.1718140840530396 -  accuracy = 0.567799985408783\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss = 1.100266695022583 -  accuracy = 0.5655999779701233\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss = 1.1267088651657104 -  accuracy = 0.5630000233650208\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss = 1.1116509437561035 -  accuracy = 0.5722000002861023\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss = 1.1445071697235107 -  accuracy = 0.5759999752044678\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss = 1.1630330085754395 -  accuracy = 0.5717999935150146\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss = 1.0973882675170898 -  accuracy = 0.5651999711990356\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss = 1.1112836599349976 -  accuracy = 0.5726000070571899\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss = 1.1150590181350708 -  accuracy = 0.5726000070571899\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss = 1.149306058883667 -  accuracy = 0.5658000111579895\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss = 1.167525291442871 -  accuracy = 0.5669999718666077\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss = 1.0851558446884155 -  accuracy = 0.5636000037193298\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss = 1.0983824729919434 -  accuracy = 0.5716000199317932\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss = 1.0899937152862549 -  accuracy = 0.576200008392334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, CIFAR-10 Batch 1:  loss = 1.1546710729599 -  accuracy = 0.5690000057220459\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss = 1.1436913013458252 -  accuracy = 0.5777999758720398\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss = 1.085411548614502 -  accuracy = 0.5672000050544739\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss = 1.084950566291809 -  accuracy = 0.5788000226020813\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss = 1.0810071229934692 -  accuracy = 0.579200029373169\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss = 1.1453856229782104 -  accuracy = 0.5726000070571899\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss = 1.1429831981658936 -  accuracy = 0.5803999900817871\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss = 1.0739444494247437 -  accuracy = 0.574999988079071\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss = 1.081634521484375 -  accuracy = 0.58160001039505\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss = 1.0823742151260376 -  accuracy = 0.5756000280380249\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss = 1.1208124160766602 -  accuracy = 0.5825999975204468\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss = 1.1280674934387207 -  accuracy = 0.5849999785423279\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss = 1.0588977336883545 -  accuracy = 0.5781999826431274\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss = 1.0809129476547241 -  accuracy = 0.5735999941825867\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss = 1.0750195980072021 -  accuracy = 0.5774000287055969\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss = 1.1401981115341187 -  accuracy = 0.574999988079071\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss = 1.1257292032241821 -  accuracy = 0.5812000036239624\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss = 1.0667215585708618 -  accuracy = 0.5738000273704529\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss = 1.0802949666976929 -  accuracy = 0.5781999826431274\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss = 1.072256326675415 -  accuracy = 0.5834000110626221\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss = 1.1256595849990845 -  accuracy = 0.5753999948501587\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss = 1.114854097366333 -  accuracy = 0.5809999704360962\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss = 1.072201132774353 -  accuracy = 0.567799985408783\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss = 1.0647374391555786 -  accuracy = 0.5835999846458435\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss = 1.0744519233703613 -  accuracy = 0.574999988079071\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss = 1.109237551689148 -  accuracy = 0.5794000029563904\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss = 1.1091704368591309 -  accuracy = 0.5830000042915344\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss = 1.0487666130065918 -  accuracy = 0.578000009059906\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss = 1.0678409337997437 -  accuracy = 0.5806000232696533\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss = 1.0660829544067383 -  accuracy = 0.5774000287055969\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss = 1.1065460443496704 -  accuracy = 0.5910000205039978\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss = 1.1002931594848633 -  accuracy = 0.5849999785423279\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss = 1.0484228134155273 -  accuracy = 0.574400007724762\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss = 1.0635203123092651 -  accuracy = 0.5789999961853027\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss = 1.0692963600158691 -  accuracy = 0.5839999914169312\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss = 1.1043007373809814 -  accuracy = 0.5821999907493591\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss = 1.1093775033950806 -  accuracy = 0.5813999772071838\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss = 1.048782229423523 -  accuracy = 0.5776000022888184\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss = 1.036139726638794 -  accuracy = 0.5875999927520752\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss = 1.053121566772461 -  accuracy = 0.5867999792098999\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss = 1.0906405448913574 -  accuracy = 0.5863999724388123\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss = 1.0926967859268188 -  accuracy = 0.5910000205039978\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss = 1.0462350845336914 -  accuracy = 0.5756000280380249\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss = 1.0488229990005493 -  accuracy = 0.5795999765396118\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss = 1.034696102142334 -  accuracy = 0.5899999737739563\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss = 1.0985971689224243 -  accuracy = 0.5861999988555908\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss = 1.0810610055923462 -  accuracy = 0.5884000062942505\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss = 1.0462700128555298 -  accuracy = 0.579800009727478\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss = 1.031869888305664 -  accuracy = 0.5964000225067139\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss = 1.0421758890151978 -  accuracy = 0.5875999927520752\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss = 1.088807463645935 -  accuracy = 0.5852000117301941\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss = 1.0995181798934937 -  accuracy = 0.5831999778747559\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss = 1.0599254369735718 -  accuracy = 0.574999988079071\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss = 1.0285718441009521 -  accuracy = 0.5906000137329102\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss = 1.0276153087615967 -  accuracy = 0.5920000076293945\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss = 1.0793439149856567 -  accuracy = 0.5827999711036682\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss = 1.0801961421966553 -  accuracy = 0.590399980545044\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss = 1.0253636837005615 -  accuracy = 0.5852000117301941\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss = 1.0501766204833984 -  accuracy = 0.5734000205993652\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss = 1.0266493558883667 -  accuracy = 0.5935999751091003\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss = 1.1006062030792236 -  accuracy = 0.571399986743927\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss = 1.0711159706115723 -  accuracy = 0.5997999906539917\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss = 1.0128607749938965 -  accuracy = 0.5911999940872192\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss = 1.0289359092712402 -  accuracy = 0.5907999873161316\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss = 1.033170461654663 -  accuracy = 0.5925999879837036\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss = 1.0676711797714233 -  accuracy = 0.5946000218391418\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss = 1.066796898841858 -  accuracy = 0.5935999751091003\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss = 1.011575698852539 -  accuracy = 0.5924000144004822\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss = 1.0135369300842285 -  accuracy = 0.597000002861023\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss = 1.0254608392715454 -  accuracy = 0.5947999954223633\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss = 1.0597940683364868 -  accuracy = 0.5965999960899353\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss = 1.0650783777236938 -  accuracy = 0.5914000272750854\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss = 1.0071810483932495 -  accuracy = 0.5896000266075134\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss = 1.0203192234039307 -  accuracy = 0.5935999751091003\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss = 1.021202564239502 -  accuracy = 0.5928000211715698\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss = 1.0558439493179321 -  accuracy = 0.5956000089645386\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss = 1.0566819906234741 -  accuracy = 0.5961999893188477\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss = 1.0316022634506226 -  accuracy = 0.5807999968528748\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss = 1.0295617580413818 -  accuracy = 0.5914000272750854\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss = 1.0226248502731323 -  accuracy = 0.5960000157356262\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss = 1.0396054983139038 -  accuracy = 0.597599983215332\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss = 1.0382330417633057 -  accuracy = 0.6000000238418579\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss = 1.0396928787231445 -  accuracy = 0.5827999711036682\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss = 1.004663348197937 -  accuracy = 0.5916000008583069\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss = 1.0238449573516846 -  accuracy = 0.5867999792098999\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss = 1.0728601217269897 -  accuracy = 0.5853999853134155\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss = 1.055570125579834 -  accuracy = 0.5932000279426575\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss = 0.9830846786499023 -  accuracy = 0.5983999967575073\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss = 1.0325233936309814 -  accuracy = 0.5795999765396118\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss = 1.0144093036651611 -  accuracy = 0.6007999777793884\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss = 1.0430463552474976 -  accuracy = 0.605400025844574\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss = 1.0340391397476196 -  accuracy = 0.6021999716758728\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss = 0.9801939129829407 -  accuracy = 0.5996000170707703\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss = 1.0025815963745117 -  accuracy = 0.5889999866485596\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss = 1.0010437965393066 -  accuracy = 0.5997999906539917\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss = 1.0428555011749268 -  accuracy = 0.597000002861023\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss = 1.0477969646453857 -  accuracy = 0.5979999899864197\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss = 0.999881386756897 -  accuracy = 0.5920000076293945\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss = 1.0016635656356812 -  accuracy = 0.5884000062942505\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss = 1.0042705535888672 -  accuracy = 0.5934000015258789\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss = 1.030023217201233 -  accuracy = 0.6037999987602234\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss = 1.0287903547286987 -  accuracy = 0.5956000089645386\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss = 0.9845694303512573 -  accuracy = 0.5935999751091003\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss = 1.004469633102417 -  accuracy = 0.5917999744415283\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss = 1.0285171270370483 -  accuracy = 0.5879999995231628\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss = 1.0260660648345947 -  accuracy = 0.605400025844574\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss = 1.0252625942230225 -  accuracy = 0.5965999960899353\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss = 0.9852377772331238 -  accuracy = 0.5964000225067139\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss = 1.007064938545227 -  accuracy = 0.5956000089645386\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss = 0.9882187843322754 -  accuracy = 0.603600025177002\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss = 1.0391185283660889 -  accuracy = 0.605400025844574\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss = 1.020880103111267 -  accuracy = 0.6028000116348267\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss = 0.9584912657737732 -  accuracy = 0.6032000184059143\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss = 0.9859426021575928 -  accuracy = 0.5961999893188477\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss = 1.0233317613601685 -  accuracy = 0.5916000008583069\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss = 1.0302181243896484 -  accuracy = 0.6069999933242798\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss = 1.0151468515396118 -  accuracy = 0.603600025177002\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss = 0.9678738117218018 -  accuracy = 0.6028000116348267\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss = 0.9938408732414246 -  accuracy = 0.5956000089645386\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss = 0.9913117289543152 -  accuracy = 0.6047999858856201\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss = 1.0209827423095703 -  accuracy = 0.604200005531311\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss = 1.0240190029144287 -  accuracy = 0.5997999906539917\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss = 0.9808487296104431 -  accuracy = 0.5942000150680542\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss = 0.9854591488838196 -  accuracy = 0.5964000225067139\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss = 0.9794925451278687 -  accuracy = 0.5982000231742859\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss = 1.0218782424926758 -  accuracy = 0.6043999791145325\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss = 0.9974837899208069 -  accuracy = 0.6055999994277954\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss = 0.9483631253242493 -  accuracy = 0.6029999852180481\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss = 0.9847801923751831 -  accuracy = 0.6029999852180481\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss = 0.9715471267700195 -  accuracy = 0.6061999797821045\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss = 1.003198504447937 -  accuracy = 0.6105999946594238\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss = 0.9959506988525391 -  accuracy = 0.6046000123023987\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss = 0.9833778738975525 -  accuracy = 0.5863999724388123\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss = 0.9839270710945129 -  accuracy = 0.5946000218391418\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss = 0.9824985265731812 -  accuracy = 0.5996000170707703\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss = 1.0042701959609985 -  accuracy = 0.6029999852180481\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss = 1.0047610998153687 -  accuracy = 0.6057999730110168\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss = 0.9551153779029846 -  accuracy = 0.6011999845504761\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss = 0.9740147590637207 -  accuracy = 0.603600025177002\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss = 0.9798784852027893 -  accuracy = 0.6068000197410583\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss = 1.001012921333313 -  accuracy = 0.6057999730110168\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss = 0.9858418703079224 -  accuracy = 0.6161999702453613\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss = 0.935404360294342 -  accuracy = 0.6050000190734863\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss = 0.957018256187439 -  accuracy = 0.6037999987602234\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss = 0.9718509912490845 -  accuracy = 0.6140000224113464\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss = 0.9944331049919128 -  accuracy = 0.6105999946594238\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss = 0.9883050918579102 -  accuracy = 0.6123999953269958\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss = 0.9218289852142334 -  accuracy = 0.6087999939918518\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss = 0.9510206580162048 -  accuracy = 0.602400004863739\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss = 0.9505974650382996 -  accuracy = 0.6029999852180481\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss = 0.9860074520111084 -  accuracy = 0.6079999804496765\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss = 0.9743703603744507 -  accuracy = 0.6119999885559082\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss = 0.9315815567970276 -  accuracy = 0.6047999858856201\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss = 0.9624908566474915 -  accuracy = 0.6011999845504761\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss = 0.9538571834564209 -  accuracy = 0.6092000007629395\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss = 0.9834105968475342 -  accuracy = 0.6060000061988831\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss = 0.97439044713974 -  accuracy = 0.6129999756813049\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss = 0.9167170524597168 -  accuracy = 0.6018000245094299\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss = 0.9626321196556091 -  accuracy = 0.6100000143051147\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss = 0.9674642086029053 -  accuracy = 0.6003999710083008\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss = 0.9854724407196045 -  accuracy = 0.6028000116348267\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss = 0.9920586943626404 -  accuracy = 0.607200026512146\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss = 0.9530958533287048 -  accuracy = 0.5979999899864197\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss = 0.9575266242027283 -  accuracy = 0.6033999919891357\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss = 0.9546160697937012 -  accuracy = 0.6033999919891357\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss = 0.9727894067764282 -  accuracy = 0.6115999817848206\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss = 0.9648256301879883 -  accuracy = 0.6065999865531921\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss = 0.9204332232475281 -  accuracy = 0.6037999987602234\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss = 0.9650294184684753 -  accuracy = 0.597599983215332\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss = 0.9498569965362549 -  accuracy = 0.6141999959945679\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss = 0.9883096814155579 -  accuracy = 0.605400025844574\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss = 0.9652690291404724 -  accuracy = 0.6087999939918518\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss = 0.923326849937439 -  accuracy = 0.6032000184059143\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss = 0.940115749835968 -  accuracy = 0.6123999953269958\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss = 0.9422950148582458 -  accuracy = 0.6097999811172485\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss = 0.9831990003585815 -  accuracy = 0.6051999926567078\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss = 0.9862914085388184 -  accuracy = 0.6101999878883362\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss = 0.9208466410636902 -  accuracy = 0.6028000116348267\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss = 0.9562318921089172 -  accuracy = 0.5996000170707703\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss = 0.9428688883781433 -  accuracy = 0.6105999946594238\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss = 0.9651164412498474 -  accuracy = 0.6087999939918518\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss = 0.9813255071640015 -  accuracy = 0.6114000082015991\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss = 0.8965479135513306 -  accuracy = 0.6128000020980835\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss = 0.9529097676277161 -  accuracy = 0.598800003528595\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss = 0.9442940354347229 -  accuracy = 0.6168000102043152\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss = 0.9592825770378113 -  accuracy = 0.6104000210762024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, CIFAR-10 Batch 2:  loss = 0.9459967017173767 -  accuracy = 0.6164000034332275\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss = 0.9033116102218628 -  accuracy = 0.6105999946594238\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss = 0.9241500496864319 -  accuracy = 0.6096000075340271\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss = 0.942514955997467 -  accuracy = 0.6079999804496765\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss = 0.9567893743515015 -  accuracy = 0.6123999953269958\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss = 0.9600926637649536 -  accuracy = 0.6164000034332275\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss = 0.9111882448196411 -  accuracy = 0.6019999980926514\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss = 0.9386377334594727 -  accuracy = 0.6065999865531921\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss = 0.9603548049926758 -  accuracy = 0.6082000136375427\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss = 0.9545969367027283 -  accuracy = 0.6146000027656555\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss = 0.9461164474487305 -  accuracy = 0.6137999892234802\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss = 0.9027774930000305 -  accuracy = 0.6105999946594238\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss = 0.9214580059051514 -  accuracy = 0.614799976348877\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss = 0.9224954843521118 -  accuracy = 0.616599977016449\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss = 0.9508802890777588 -  accuracy = 0.621399998664856\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss = 0.937966525554657 -  accuracy = 0.625\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss = 0.8932863473892212 -  accuracy = 0.6093999743461609\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss = 0.9334721565246582 -  accuracy = 0.6114000082015991\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss = 0.926722526550293 -  accuracy = 0.6092000007629395\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss = 0.9535863995552063 -  accuracy = 0.6223999857902527\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss = 0.9389417171478271 -  accuracy = 0.6223999857902527\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss = 0.886728823184967 -  accuracy = 0.6104000210762024\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss = 0.9642843008041382 -  accuracy = 0.5997999906539917\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss = 0.933807373046875 -  accuracy = 0.6136000156402588\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss = 0.9672605395317078 -  accuracy = 0.6119999885559082\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss = 0.92733234167099 -  accuracy = 0.6218000054359436\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss = 0.9102064371109009 -  accuracy = 0.618399977684021\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss = 0.9129666090011597 -  accuracy = 0.6086000204086304\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss = 0.931785523891449 -  accuracy = 0.6114000082015991\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss = 0.9484912157058716 -  accuracy = 0.6118000149726868\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss = 0.9390789866447449 -  accuracy = 0.6212000250816345\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss = 0.891793966293335 -  accuracy = 0.6144000291824341\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss = 0.9089658856391907 -  accuracy = 0.6129999756813049\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss = 0.9104452729225159 -  accuracy = 0.6187999844551086\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss = 0.9417217969894409 -  accuracy = 0.623199999332428\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss = 0.9190024137496948 -  accuracy = 0.6230000257492065\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss = 0.8915825486183167 -  accuracy = 0.604200005531311\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss = 0.9084689617156982 -  accuracy = 0.6186000108718872\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss = 0.9149556756019592 -  accuracy = 0.6146000027656555\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss = 0.9390939474105835 -  accuracy = 0.6200000047683716\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss = 0.9247944951057434 -  accuracy = 0.6197999715805054\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss = 0.8793649077415466 -  accuracy = 0.6140000224113464\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss = 0.9481265544891357 -  accuracy = 0.5978000164031982\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss = 0.9270626902580261 -  accuracy = 0.6173999905586243\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss = 0.9625406265258789 -  accuracy = 0.6093999743461609\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss = 0.9207984805107117 -  accuracy = 0.6161999702453613\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss = 0.8847341537475586 -  accuracy = 0.6179999709129333\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss = 0.9119211435317993 -  accuracy = 0.6043999791145325\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss = 0.9144725799560547 -  accuracy = 0.6191999912261963\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss = 0.9490067362785339 -  accuracy = 0.6169999837875366\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss = 0.9154687523841858 -  accuracy = 0.626800000667572\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss = 0.8709555864334106 -  accuracy = 0.6205999851226807\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss = 0.9006310701370239 -  accuracy = 0.616599977016449\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss = 0.9250426292419434 -  accuracy = 0.6150000095367432\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss = 0.9169295430183411 -  accuracy = 0.6227999925613403\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss = 0.9225108027458191 -  accuracy = 0.6177999973297119\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss = 0.8588573932647705 -  accuracy = 0.6215999722480774\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss = 0.8968816995620728 -  accuracy = 0.6115999817848206\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss = 0.8998972177505493 -  accuracy = 0.6190000176429749\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss = 0.933091402053833 -  accuracy = 0.6164000034332275\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss = 0.9058851003646851 -  accuracy = 0.6237999796867371\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss = 0.8640627861022949 -  accuracy = 0.6179999709129333\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss = 0.900147557258606 -  accuracy = 0.6118000149726868\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss = 0.8890094757080078 -  accuracy = 0.6187999844551086\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss = 0.9175409078598022 -  accuracy = 0.6245999932289124\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss = 0.8946989178657532 -  accuracy = 0.6240000128746033\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss = 0.8473740220069885 -  accuracy = 0.6240000128746033\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss = 0.8846789002418518 -  accuracy = 0.6236000061035156\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss = 0.888051450252533 -  accuracy = 0.6223999857902527\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss = 0.9195057153701782 -  accuracy = 0.6141999959945679\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss = 0.9062831401824951 -  accuracy = 0.621999979019165\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss = 0.8863709568977356 -  accuracy = 0.61080002784729\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss = 0.9124581217765808 -  accuracy = 0.6092000007629395\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss = 0.8844155669212341 -  accuracy = 0.6240000128746033\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss = 0.930303692817688 -  accuracy = 0.628600001335144\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss = 0.8829509019851685 -  accuracy = 0.6263999938964844\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss = 0.8500410914421082 -  accuracy = 0.6236000061035156\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss = 0.8974591493606567 -  accuracy = 0.618399977684021\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss = 0.8895982503890991 -  accuracy = 0.6097999811172485\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss = 0.947127640247345 -  accuracy = 0.6251999735832214\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss = 0.8971543312072754 -  accuracy = 0.623199999332428\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss = 0.8478320240974426 -  accuracy = 0.6226000189781189\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss = 0.8717419505119324 -  accuracy = 0.6258000135421753\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss = 0.8711211681365967 -  accuracy = 0.6209999918937683\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss = 0.9091860055923462 -  accuracy = 0.6263999938964844\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss = 0.8943456411361694 -  accuracy = 0.6197999715805054\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss = 0.8479444980621338 -  accuracy = 0.6186000108718872\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss = 0.8741355538368225 -  accuracy = 0.6173999905586243\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss = 0.8710843920707703 -  accuracy = 0.6222000122070312\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss = 0.9176045656204224 -  accuracy = 0.6240000128746033\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss = 0.8900318741798401 -  accuracy = 0.6222000122070312\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss = 0.8546693921089172 -  accuracy = 0.6204000115394592\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss = 0.8841267824172974 -  accuracy = 0.6110000014305115\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss = 0.8626191020011902 -  accuracy = 0.6226000189781189\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss = 0.9071345329284668 -  accuracy = 0.628000020980835\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss = 0.8806897401809692 -  accuracy = 0.6338000297546387\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss = 0.8540313243865967 -  accuracy = 0.6187999844551086\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss = 0.8639379739761353 -  accuracy = 0.6168000102043152\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss = 0.8798898458480835 -  accuracy = 0.6215999722480774\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss = 0.9052884578704834 -  accuracy = 0.6176000237464905\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss = 0.8903017044067383 -  accuracy = 0.6212000250816345\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss = 0.8519301414489746 -  accuracy = 0.6197999715805054\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss = 0.864536702632904 -  accuracy = 0.6194000244140625\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss = 0.8762022256851196 -  accuracy = 0.6236000061035156\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss = 0.9024708271026611 -  accuracy = 0.6205999851226807\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss = 0.8881338238716125 -  accuracy = 0.6317999958992004\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss = 0.8555704355239868 -  accuracy = 0.6304000020027161\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss = 0.867867648601532 -  accuracy = 0.6194000244140625\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss = 0.8571458458900452 -  accuracy = 0.6263999938964844\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss = 0.891570508480072 -  accuracy = 0.6200000047683716\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss = 0.8775900602340698 -  accuracy = 0.6237999796867371\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss = 0.8504037261009216 -  accuracy = 0.6241999864578247\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss = 0.8901984095573425 -  accuracy = 0.6068000197410583\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss = 0.8522862195968628 -  accuracy = 0.6273999810218811\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss = 0.8889191746711731 -  accuracy = 0.6236000061035156\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss = 0.8832675218582153 -  accuracy = 0.6195999979972839\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss = 0.8409683704376221 -  accuracy = 0.6281999945640564\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss = 0.8608387112617493 -  accuracy = 0.6141999959945679\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss = 0.85696941614151 -  accuracy = 0.6277999877929688\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss = 0.893378734588623 -  accuracy = 0.6168000102043152\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss = 0.8638811707496643 -  accuracy = 0.6244000196456909\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss = 0.8482199311256409 -  accuracy = 0.61080002784729\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss = 0.8679823875427246 -  accuracy = 0.6187999844551086\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss = 0.8512271046638489 -  accuracy = 0.6237999796867371\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss = 0.9009588360786438 -  accuracy = 0.6233999729156494\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss = 0.8763588070869446 -  accuracy = 0.6335999965667725\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss = 0.8314844965934753 -  accuracy = 0.6273999810218811\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss = 0.8485274314880371 -  accuracy = 0.6248000264167786\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss = 0.8701299428939819 -  accuracy = 0.6262000203132629\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "模型已保存到本地。\n",
    "\n",
    "## 测试模型\n",
    "\n",
    "利用测试数据集测试你的模型。这将是最终的准确率。你的准确率应该高于 50%。如果没达到，请继续调整模型结构和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6228993952274322\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpEmxyIiIiIi0f8DmseM2zCiaMMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xccaa080>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为何准确率只有50-80%？\n",
    "\n",
    "你可能想问，为何准确率不能更高了？首先，对于简单的 CNN 网络来说，50% 已经不低了。纯粹猜测的准确率为10%。但是，你可能注意到有人的准确率[远远超过 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)。这是因为我们还没有介绍所有的神经网络知识。我们还需要掌握一些其他技巧。\n",
    "\n",
    "## 提交项目\n",
    "\n",
    "提交项目时，确保先运行所有单元，然后再保存记事本。将 notebook 文件另存为“dlnd_image_classification.ipynb”，再在目录 \"File\" -> \"Download as\" 另存为 HTML 格式。请在提交的项目中包含 “helper.py” 和 “problem_unittests.py” 文件。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
